# -*- coding: utf-8 -*-
"""Copy of ScratchNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10y_79hiv3mFwDyWbqsqBsGs9f72D86wy
"""

import random
import numpy as np
from keras.datasets import mnist

class Network:
    def __init__(self, layers):
        """ Randomises the weights and biases """
        self.layers = layers
        self.weights = []
        self.biases = []
        # initialise the layers
        for layer in range(1, len(layers)):
            # connections
            layer_w = []
            for node in range(layers[layer]):
                layer_w.append([np.random.randn() for i in range(layers[layer-1])])
                
            layer_b = [np.random.randn() for i in range(layers[layer])]
            self.weights.append(layer_w)
            self.biases.append(layer_b)

    def forward_propagation(self, inputs, layer=None):
        """ gets the end nodes """
        if (layer == None): layer = len(self.layers)-1
        activations = inputs
        for i in range(layer):
            activations = [np.dot(activations, w) for w in self.weights[i]]
            activations = list(np.array(activations) + np.array(self.biases[i]))
            activations = [sigmoid(z) for z in activations]
        return activations

    def train(self, data, rotations, batch_size, lr, test_data):
        """ Trains the network """
        for i in range(rotations):
            # divide the data
            random.shuffle(data)
            batches = [data[k:k+batch_size] 
                       for k in range(0, len(data), batch_size)]
            # train on the batches
            print(len(batches))
            for j in range(len(batches)):
                self.batch_train(batches[j], lr)
                correct, total = self.test(test_data)
                print("Rot {0}, Batch {1}: {2}/{3}".format(i+1, j, correct, total))

    def batch_train(self, data, lr):
        """ trains the network on a small batch """
        deltaw_sum = [np.zeros(np.array(w).shape).tolist() for w in self.weights]
        deltab_sum = [np.zeros(np.array(b).shape).tolist() for b in self.biases]
        # add to the sum lists for each datapoint
        for point, ans in data:
            dl_dw, dl_db = self.backward_propagation(point, ans)
            deltaw_sum = [sum+dl for sum, dl in zip(deltaw_sum, dl_dw)]
            deltab_sum = [sum+dl for sum, dl in zip(deltab_sum, dl_db)]

        # apply gradient decent with the average delta
        self.biases = [list(np.array(b)-(lr/len(data))*nb)
                       for b, nb in zip(self.biases, deltab_sum)]


        for i in range(len(self.weights)):
            for j in range(len(self.weights[i])):
                for k in range(len(self.weights[i][j])):
                    self.weights[i][j][k] -= (lr/len(data))*deltaw_sum[i][j][k]

    def backward_propagation(self, act, out):
        """ Returns the change in weights and biases for one datapoint """
        deltaw = []
        deltab = []

        # get the forward propagation of each layer
        activations = [act]
        for i in range(1, len(self.layers)):
            activations.append(self.forward_propagation(act, i))

        # calculate the cost in respect to activation for the last layer
        dc_dal = [2*(a - y) for a, y in zip(act, out)]           # cost in respect of activations of next layer
        da_dz = [sigmoid_prime(i) for i in activations[-1]]
        
        # calculate for the rest of the layers
        for i in range(len(self.layers)-2, -1, -1):
            dc_dz = [a*b for a, b in zip(dc_dal, da_dz)]

            # stores delat for that layer
            dw_l = []
            db_l = dc_dz

            dc_dal1 = []
            # sum the delta for each neuron in following neuron
            for j in range(self.layers[i]):
                w = [k[j] for k in self.weights[i]]

                # change in cost with respect to activations
                dc_dal1.append(
                    np.dot(
                        np.array(w), 
                        np.array(dc_dz)
                    )
                )
                # change in cost with respect to weights
                dw_l.append([a*c for a, c in zip(activations[i], dc_dz)])

            # stores the delta
            deltaw = dw_l + deltaw
            deltab = db_l + deltab
            
            # set variables for next layer
            dc_dal = dc_dal1
            da_dz = [sigmoid_prime(a) for a in activations[i]]

        return (deltaw, deltab)


    def test(self, data):
        """ tests the data to see the success rate """
        correct = 0
        total = 0
        for point, ans in data:
            total += 1
            if self.check_results(self.forward_propagation(point), ans):
                correct += 1
        return (correct, total)

    def check_results(self, out1, out2):
        """ check if the output of both are both the same """
        return out1.index(max(out1)) == out2.index(max(out2))

def sigmoid(z):
    """The sigmoid function."""
    return 1.0/(1.0+np.exp(-z))

def sigmoid_prime(z):
    """Derivative of the sigmoid function."""
    return sigmoid(z)*(1-sigmoid(z))

def format_data(data):
    """ format data """
    new_data = []
    for image, num in zip(data[0], data[1]):
        entry = list(np.array(image).flatten())
        score = list(np.zeros(10))
        score[num] = 1
        new_data.append((entry, score))
    return new_data

if __name__ == "__main__":
    training_data, test_data = mnist.load_data()
    training_data = format_data(training_data)
    test_data = format_data(test_data)
    net = Network([784, 30, 10])
    #train(self, data, rotations, batch_size, lr, test_data)
    net.train(training_data, 10, 16, 1, test_data[:200])

